{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aabf6a7e-2dd6-4b09-9b52-96a06eee3979",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLTK Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb65fd9-e9dc-497f-b397-9178cd3ed9d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing NLTK installation and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c33152-cc8b-4d1c-bc3a-03ff0e87b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5a82fe-89af-47ea-9184-3f15910ddcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c289e85-7529-4a84-a1e4-9e600a9e7203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Loading all items from NLTK's book module\n",
    "\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f005e853-b4ed-4942-942b-8e8616e8285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring brown corpus\n",
    "\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3b4ad2-e02f-4674-8334-9bc9fbd289b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'is', 'not', 'news', 'that', 'Nathan', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_words = brown.words(categories=\"reviews\")\n",
    "reviews_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c02192-9eba-4129-87f3-a56967b0ee8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a8dc0-87a1-4e3f-be0d-5dd8921d8668",
   "metadata": {},
   "source": [
    "## Text Extraction and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060067d9-c200-48be-ad15-7b3d6016ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World!', 'This is sentence tokenizing.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenizing\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello World! This is sentence tokenizing.\"\n",
    "print(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd43e9b3-dd4d-4968-b3df-f3be9c7d1ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!', 'This', 'is', 'word', 'tokenizing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenizing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "EXAMPLE_WORD = \"Hello World! This is word tokenizing.\"\n",
    "print (word_tokenize(EXAMPLE_WORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e00e494-171e-4dbc-aa7f-a0665fdc4d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english')) # filtered out before the NLP takes place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a963222a-e742-4d34-9578-f7fa2a8f17ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'day', 'keeps', 'diseases', 'bay', '.']\n"
     ]
    }
   ],
   "source": [
    "# Using Stop Words to remove unnecessary words from example text\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"an Apple a day keeps diseases at bay.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05ce4dd9-a7a7-444c-95bd-a6b1f225e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import\n",
      "of\n",
      "cave\n",
      "as\n",
      "explain\n",
      "by\n",
      "caver\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "#from nitk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "new_text = \"importance of caving as explained by cavers\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3444b35-7dd9-4888-b138-e947c597eb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "cactus\n",
      "goose\n",
      "loving\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "                           \n",
    "# Without a POS tag, Lemmatizer assumes everything is a noun\n",
    "                           \n",
    "print(lemmatizer.lemmatize(\"loving\"))\n",
    "print(lemmatizer.lemmatize(\"loving\", 'v')) # with POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85e1a91a-f31f-44dd-b2d7-8c40ca5b10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Text', 'NNP'), ('mining', 'NN'), (',', ','), ('also', 'RB'), ('referred', 'VBD'), ('text', 'JJ'), ('data', 'NN'), ('mining', 'NN'), (',', ','), ('roughly', 'RB'), ('equivalent', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), (',', ','), ('process', 'NN'), ('deriving', 'VBG'), ('high-quality', 'NN'), ('information', 'NN'), ('text', 'NN'), ('.', '.')]\n",
      "[('High-quality', 'NNP'), ('information', 'NN'), ('typically', 'RB'), ('derived', 'VBD'), ('devising', 'VBG'), ('patterns', 'NNS'), ('trends', 'NNS'), ('means', 'VBZ'), ('statistical', 'JJ'), ('pattern', 'NN'), ('learning', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Text Extraction / POS Tagging\n",
    "\n",
    "#stop words = set(stopwords.words('english))\n",
    "txt = '''\n",
    "    Text mining, also referred to as text data mining,\n",
    "    roughly equivalent to text analytics, is the process of deriving high-quality information from text.\n",
    "    High-quality information is typically derived through the devising of patterns and trends through means\n",
    "    such as statistical pattern learning.\n",
    "    '''\n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    \n",
    "    print (tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2954daca-8d82-4b9f-b186-8a66ec33976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', 'GPE'), ('American', 'GPE'), ('Larry Page', 'PERSON'), ('Sergey Brin', 'PERSON'), ('Stanford University', 'ORGANIZATION'), ('California', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Text Extraction and pre-processing - Named Entity Recognition (NER)\n",
    "\n",
    "#import nltk\n",
    "doc = '''Google is an American multinational technology company that specializes in related \n",
    "services and products, which include online advertising technologies, search engine, cloud \n",
    "computing, and hardware. it was founded in 1998 by Larry Page and Sergey Brin while they were \n",
    "Ph.D. students at Stanford University in California'''\n",
    "\n",
    "# tokenize doc\n",
    "tokenized_doc = nltk.word_tokenize(doc)\n",
    "tagged_sentences = nltk.pos_tag(tokenized_doc)\n",
    "ne_chunked_sents = nltk.ne_chunk(tagged_sentences)\n",
    "\n",
    "# extract all named entities\n",
    "named_entities = []\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    if hasattr(tagged_tree, \"label\"):\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves())\n",
    "        entity_type = tagged_tree.label()\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c3e80-9bc6-4142-a0c6-4b71c951633c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
